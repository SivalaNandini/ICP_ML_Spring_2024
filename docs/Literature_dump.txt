Please use this text file to dump out your literature so that we can reorganize it into a word file once everyone has completed their part.

       Sub-task                 Member              Status

Task Description:               Mohammed             TBD
Expected Challenges (15%):      Colton               TBD
Significance of Project (10%):  Karthik              DONE
Related work (10%):             Nandini              DONE
Data description (25%):         Teja                 DONE 
Proposed approach (10%):        Chaithanya           TBD
Final review (10%):             Hema                 TBD
Images and graph inclusion:     Owais                TBD
 
Title of the project (5%) (Please feel free to suggest any other title you feel would be good)
Suggestions so far: Image Caption Generator (ICG), Image captioning with Machine Learning (ICML)

4 pages max (10%)
single column & 12 font size (5%)

Task Description:

Expected Challenges (15%):

Significance of Project (10%):
Image caption generator converts a set of pixel data into a human readable form. It is a very significant problem in computer vision that helps the user in scene understanding. It helps in providing additional information about a given image to get a better understanding of the image. Generating a caption to describe the interaction between elements in an image can be very helpful in so may ways. These are a few examples where Image caption generator could be useful, (i) It can help the visually impaired people to listen to the captions and understand them without having to view the image or touching it; (ii) It can also help in content recognition and recommendation for similar images; and (iii) It can be used to improve the quality of image based websites and applications.

Related work (10%): Sivala Nandini(11755819)
"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention," which was put forth by Yoshua Bengio, Richard S. Zemel, Kyunghyun Cho, Aaron Courville, Jimmy Lei Ba, Ryan Kiros, Ruslan Salakhutdinov, and Kelvin Xu in 2016, presents attention-based models for automatically describing image content. incorporates attention methods to generate innovative picture captions that are more accurate in describing images by dynamically focusing on prominent features. Inspired by machine translation and object recognition, this method achieves state-of-the-art results on benchmark datasets such as MS COCO, Flickr8k, and Flickr30k. Enhanced caption quality, attention visualization, and insights into model behavior are among its strengths. Nevertheless, problems are brought about by the attention processes' introduction of training variability and increasing model complexity. To guarantee stability and convergence, managing this complexity necessitates cautious application and training methodologies. "Show, Attend and Tell" is a potential way to significantly improve the interpretation and expression of visual content in natural language, and it marks a substantial development in image captioning overall.

The Neural Image Caption (NIC) model is introduced in "Show and Tell: A Neural Image Caption Generator," a proposal that Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan made in 2015. It addresses the problem of automatically describing images with natural language. NIC provides an end-to-end solution trainable via stochastic gradient descent by fusing recurrent neural networks (RNNs) for language synthesis with deep convolutional neural networks (CNNs) for picture understanding. Higher BLEU scores on several datasets demonstrate its remarkable performance advantages over existing methods due to its inclusion of cutting-edge sub-networks for language and vision models. Even with its advantages, NIC may have drawbacks due to its intricacy and need on large amounts of training data. Its performance could be limited by the quantity and caliber of training datasets, and its architecture could be difficult to deploy and train. Furthermore, even though BLEU ratings provide a quantitative assessment, the subtleties of created descriptions could not be entirely captured by them. Overall, NIC represents a promising advancement in the field of image captioning, though ongoing research is necessary to address its complexities and enhance its robustness across diverse domains and datasets.

Data description (25%):
https://myunt-my.sharepoint.com/:w:/g/personal/saitejanarravenkata_my_unt_edu/Eb3qCgZptMVGkD85cG99PXsBC1xw6vh3ImkOCyTVDgtVpg?e=w2zKAG
Proposed approach (10%):
